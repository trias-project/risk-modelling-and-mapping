---
title: "Untitled"
author: "Amy Davis", 
        "Soria Delva"
date: "2024-08-20"
output: html_document
---
#Line 228 - 536 script Soria
### 2. Create a global SDM 
##### 2. Specify paths for output (defaults to file structure in ReadMe)
```{r defineOutputPaths, echo=FALSE}
rasterOutput<-here("data/processed/geotiffs/")
pdfOutput<-here("data/processed/pdf/")
genOutput<-here("data/processed/general//")
```
####3. Filter global occurrence data 
```{r decimal places,echo=FALSE}
#This function calculates the number of decimal places in any given numeric value 
# eg., 15.21 has 2 decimal places, 15.2569 has 4 decimal places, 15.25690 also has 4, as 0 in the end doesn't count
decimalplaces <- function(x) {
  if (abs(x - round(x)) > .Machine$double.eps^0.5) {
    nchar(strsplit(sub('0+$', '', as.character(x)), ".", fixed = TRUE)[[1]][[2]])
  } else {
    return(0)
  }
}
```

```{r filter global occurrence data}

#remove unverified records
identificationVerificationStatus_to_discard <- c( "unverified",
                                                  "unvalidated",
                                                  "not validated",
                                                  "under validation",
                                                  "not able to validate",
                                                  "control could not be conclusive due to insufficient knowledge",
                                                  "1",
                                                  "uncertain",
                                                  "unconfirmed",
                                                  "Douteux",
                                                  "Invalide",
                                                  "Non r\u00E9alisable",
                                                  "verification needed" ,
                                                  "Probable",
                                                  "unconfirmed - not reviewed",
                                                  "validation requested")

#enter value for max coordinate uncertainty in meters.
global.occ<-global %>%
  dplyr::filter(speciesKey%in%taxon_key) %>%   
  dplyr::filter(is.na(coordinateUncertaintyInMeters)| coordinateUncertaintyInMeters<= 1000) %>%
  dplyr::filter(!str_to_lower(identificationVerificationStatus) %in% identificationVerificationStatus_to_discard)

#Remove coordinates that for both lon and lat values, have less than 4 decimal places
 global.occ$lon_dplaces<-sapply(global.occ$decimalLongitude, function(x) decimalplaces(x))
 global.occ$lat_dplaces<-sapply(global.occ$decimalLatitude, function(x) decimalplaces(x))
 global.occ[global.occ$lon_dplaces < 4 & global.occ$lat_dplaces < 4 , ]<-NA
 global.occ<-global.occ[ which(!is.na(global.occ$lon_dplaces)),]
 global.occ<-within(global.occ,rm("lon_dplaces","lat_dplaces")) # n= 1758
 
``` 

#### Convert global occurrences to spatial points needed for modelling

```{r occ to spatialData,echo=FALSE}
global.occ.LL<-data.frame(global.occ)[c(99:98)] #extract long and lat, n= 1758
```

#### Flag and remove centroids and invalid georeferenced points
```{r remove invalid_pts_and_centroids,echo=FALSE}
global.occ.LL$species<-rep("Vaccinium corymbosum",nrow(global.occ.LL))  

# OPTIONAL: Coordinates are tested for several things: whether they are in capitals, whether ... . For each coordinate a column per test is added indicating wether the result is potentially problematic (FALSE) or a clean coordinate (TRUE)
#flags_report<-clean_coordinates(x = global.occ.LL, lon= "decimalLongitude", lat= "decimalLatitude",
                        #  tests = c("capitals", 
                         # "centroids","gbif", "institutions", 
                          # "seas", "zeros"))

# Only keep coordinates that are not flagged as potentially problematic
cleaned<-clean_coordinates(x = global.occ.LL, lon= "decimalLongitude", lat= "decimalLatitude",
                          tests = c("capitals", 
                          "centroids","gbif", "institutions", 
                           "seas", "zeros"),value="clean")
global.occ.LL.cleaned<-subset(cleaned,select= -c(species)) #n=1692
```
#### Create global rasterstack using CHELSA data for model building

```{r importChelsaData,echo=TRUE}
globalclimrasters <- list.files((here("./data/external/climate/trias_CHELSA")),pattern='tif',full.names = T) #import CHELSA data
globalclimpreds <- stack(globalclimrasters)
```

#### Remove duplicates per grid cell and define number of pseudoabsences

```{r remove global duplicates,echo=TRUE}
globalclimpreds_terra <- terra::rast(globalclimrasters)
global.occ.LL.cleaned$cell<-terra::cellFromXY( globalclimpreds_terra, global.occ.LL.cleaned) #Indicate for each occurrence point in which cell of the raster it falls
unique_occurrences <- !duplicated(global.occ.LL.cleaned$cell)# Identify unique occurrences
global.occ.LL.cleaned <- global.occ.LL.cleaned[unique_occurrences, 1:2] # Subset the occurrence points to keep only one occurrence per raster cell 
global.occ.LL.cleaned$species<- rep(1,length(global.occ.LL.cleaned$decimalLongitude)) #adds columns indicating species presence needed for modeling
global.occ.sf<-st_as_sf(global.occ.LL.cleaned, coords=c("decimalLongitude", "decimalLatitude"), crs=4326, remove= FALSE)
numb.global.pseudoabs <-length(global.occ.sf$decimalLongitude) #sets the number of pseudoabsences equal to number of unique presences

```
### plot distribution of cleaned global occurrences
```{r plot_global_occ,echo=TRUE}
world<-ne_countries(scale=50)

ggplot()+ 
  geom_sf(data = world,  colour = "black", fill = NA)+
  geom_point(data=global.occ.sf, aes(x=decimalLongitude, y= decimalLatitude),  fill="green", shape = 22, colour = "black", size=3)+
  labs(x="Longitude", y="Latitude")+
  theme_bw()
```
### Select wwf ecoregions that contain global occurrence points

```{r select ecoregions,cache= TRUE,echo=FALSE}
wwf_eco<-sf::st_read(here("./data/external/GIS/official/wwf_terr_ecos.shp"))
wwf_eco<-st_transform(wwf_eco, 4326) %>%
          st_make_valid()
occ_ecoIntersect <- st_intersects(wwf_eco,global.occ.sp) 
wwf_ecoSub1<-wwf_eco[lengths(occ_ecoIntersect) > 0,1]
```

### Inspect the ecoregions object

```{r visualize ecoregions,cache= TRUE,echo=FALSE}
ggplot()+ 
  geom_sf(data = world,  colour = "black", fill = NA)+
  geom_sf(data=wwf_ecoSub1, fill="#f7786f")+
  labs(x="Longitude", y="Latitude")+
  theme_bw()
```

### Specify and import bias grids for relevant taxonomic group (e.g vascular plants) 

```{r importBiasgrid}
biasgrid<-raster(here("./data/external/bias_grids/final/trias/plants_1deg_min5.tif"))### specify appropriate bias grid here

```

### Subset bias grid by ecoregions containing occurrence points

```{r subsetBiasgrid}
#Mask biasgrid by ecoregions containing occurrence points
biasgrid<-terra::rast(biasgrid) #Convert biasgrid to SpatRaster object
wwf_ecoSub1_ext<-terra::ext(wwf_ecoSub1) 
wwf_ecoSub1_vector <- vect(wwf_ecoSub1) #Convert wwf_ecoSub1 to a SpatVector that can be used for masking
biasgrid_crop <- terra::crop(biasgrid, wwf_ecoSub1_ext) #Crop biasgrid to extent wwf_ecoSub1
biasgrid_sub <- terra::mask(biasgrid_crop, wwf_ecoSub1_vector)#Mask cropped biasgrid with SpatVector

#Mask biasgrid with one of the climatic layers, to make sure it doesn't extend beyond them
climategrid_rast<-terra::crop(rast(globalclimpreds@layers[[1]]), wwf_ecoSub1_ext)
biasgrid_sub<-terra::mask(biasgrid_sub, climategrid_rast) 

#Visualize
ggplot()+ 
  geom_sf(data = world,  colour = "black", fill = NA)+
  geom_spatraster(data=biasgrid_sub)+
  scale_fill_continuous(na.value = "transparent",low = "blue", high = "orange")+
  labs(x="Longitude", y="Latitude")+
  theme_bw()
 
biasgrid_sub <- raster(biasgrid_sub) #Convert SpatRaster back to normal raster object
```

###  Use randomPoints function from dismo package to locate pseduobasences within the bias grid subset 

```{r locatePseudo_absences}
# generates pseudo absences equal to (or close to) the number of presences, from cells that are not NA in biasgrid_sub and not from cells that have occurrence points as indicated in global.occ.sp. extf 1.1 increases the size of extent with 5% at each side of the extent (default value) but when ext is NULL it won't do anything.excludep = TRUE indicates that presence points are excluded from the background, prob: if TRUE the values in mask are interpreted as probability weights (only works for rasters with a modest size that can be loaded into RAM)

set.seed(728)
global_points<-dismo::randomPoints(biasgrid_sub,numb.global.pseudoabs, st_drop_geometry(global.occ.sp), ext=NULL, extf=1.1, excludep=TRUE, prob=FALSE, cellnumbers=FALSE, tryf=70, warn=2, lonlatCorrection=TRUE) 
# will throw a warning if randomPoints generated is less than numb.pseudoabs. If this happens, increase the number of tryf or ignore bias grid and sample from ecoregion only.
```

### OPTIONAL: Sample from ecoregion only
#### run if the bias grid subset of ecoregions results in too small of an area for sampling
```{r locatePseudo_absences_inEcoregions}
#  wwf_grid<-raster(here("./data/external/GIS/wwf_ecoregions_v1.tif"))
#  ecoregions_raster<-mask(wwf_grid,wwf_ecoSub1)
#  set.seed(768)
#  global_points<-randomPoints(ecoregions_raster, numb.pseudoabs, global.occ.sp, ext=NULL, extf=1.1, excludep=TRUE, prob=FALSE, cellnumbers=FALSE, tryf=150, warn=2, lonlatCorrection=TRUE) 
```

### Extract generated pseudo absences and create presence-pseudobasence dataset 

```{r create_presence_absence_dataset}
global_pseudoAbs<-global_points %>%
  as.data.frame()%>%
  mutate(species = rep(0,length(x)))%>%
  st_as_sf(coords=c("x", "y"), crs=4326, remove=FALSE)%>%
  dplyr::rename(decimalLongitude=x,
         decimalLatitude=y)

global_presabs<- rbind(global.occ.sp,global_pseudoAbs)# join pseudoabsences with presences 

```

### Inspect the presences and pseudoabsences

```{r visualize presences and pseudoabsences,cache= TRUE,echo=FALSE}
mapview(biasgrid_sub, 
        col.regions = colorRampPalette(c("blue", "orange")),
        alpha=1, 
        na.color = "transparent", 
        layer.name = "Bias Grid") +
mapview(global_presabs, zcol = "species", 
        col.regions = c("red", "blue"),
        layer.name = "Species distribution")
```

### Extract climate data for global scale modelling

```{r extractClimateData,message=FALSE}
#Note that it is possible that pseudoabsences were generated in a lake or along the coast
#If they don't fall in climatic pixels, they will be removed, leading to a few less pseudoabsence records than presence records
global.data <- sdm::sdmData(species~.,train=vect(global_presabs),predictors=rast(globalclimpreds)) 
global.data.df<-as.data.frame(global.data)

```

### Identify highly correlated predictors

```{r identifyCorrelatedPreds,echo=FALSE}
correlationMatrix<-cor(global.data.df[,-c(1,2)]) #Calculate pearson correlation among environmental values
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.7,exact=TRUE,names=TRUE)#Returns names of environmental variables to be removed because they are correlated more than 0.7 with other variables.  If two variables have a high correlation, the function removes the variable with the largest mean absolute correlation.
preds<-as.data.frame(highlyCorrelated)
kable(preds) %>%
  kable_styling(bootstrap_options = c("striped"))
```

### Remove highly correlated predictors from dataframe 

```{r removeCorrelatedPreds,echo=FALSE,message=FALSE,warning=FALSE}
global.data.df.subset<- global.data.df %>%
  select (-all_of(highlyCorrelated), -rID) %>% 
  mutate(species = as.factor(species)) %>%
  mutate(species = recode_factor(species, 
                '0' = "absent",
                '1' = "present")) %>%  # Later steps require non numeric dependent variable
  mutate(species = relevel(species, ref = "present")) 
```
### Correct global clim preds values from integer format

```{r correctPreds,echo=FALSE}
divide10<-function(x){
  value<-x/10
  return(value)
}


global.data.df.uncor<-cbind("species"=  global.data.df.subset$species,divide10(global.data.df.subset[,-c(1)]))
```



### Use caretList from Caret package to run multiple machine learning models

```{r run_globalModel,cache=TRUE,results= 'hide',message=FALSE,warning=FALSE}
#preProc: preprocessing of predictors (environmental data). method = "center" subtracts the mean of the predictor's data (again from the data in x) from the predictor values while method = "scale" divides by the standard deviation.
control <- caret::trainControl(method="cv",number=10,savePredictions="final", preProc=c("center","scale"),classProbs=TRUE)
classList1 <- c("glm","gbm","rf","earth")
set.seed(457)
global_train <- caretList(
  species~., data= global.data.df.uncor,
  trControl=control,
    methodList=classList1)
```

```{r print_globalModelAccuracy,warning=FALSE}
GlobalModelResults<-resamples(global_train) #Returns the results for each model 
Global.Mod.Accuracy<-summary(GlobalModelResults)# displays accuracy of each model
kable(Global.Mod.Accuracy$statistics$Accuracy,digits=2) %>%
kable_styling(bootstrap_options = c("striped"))
```

```{r print_globalModelKappa,warning=FALSE}
GlobalModelResults<-resamples(global_train)
kable(Global.Mod.Accuracy$statistics$Kappa,digits=2) %>%
kable_styling(bootstrap_options = c("striped"))
```

```{r print_globalModelCorrelation,warning=FALSE}
Global.Mod.Cor<-modelCor(resamples(global_train))# shows correlation among models.Weakly correlated algorithms are persuasive for stacking them in ensemble.
kable(Global.Mod.Cor,digits=2)%>%
  kable_styling(bootstrap_options = c("striped"))
```
### Create ensemble model (combine individual models into one) 
```{r run global_ensemble,TRUE}
set.seed(478)
global_stack <- caretEnsemble(
  global_train, 
  trControl=trainControl(method="cv",
                         number=10,
                         savePredictions= "final",classProbs=TRUE ))
print(global_stack)
```

### Function to return threshold where sens=spec from caret results 
```{r run accuracy_funcs}
findThresh<-function(df){
  df<-df[c("rowIndex","obs","present")]
  df<-df %>%
    mutate(observed= ifelse(obs == "present",1,0)) %>%
    select(rowIndex,observed,predicted=present)
  result<-PresenceAbsence::optimal.thresholds(df,opt.methods = 2)
  return(result)
}

#accuracy measures
accuracyStats<-function(df,y){
  df<-df[c("rowIndex","obs","present")]
  df<-df %>%
    mutate(observed= ifelse(obs == "present",1,0)) %>%
    select(rowIndex,observed,predicted=present)
  result<-PresenceAbsence::presence.absence.accuracy(df,threshold = y,st.dev=FALSE)
  return(result)
}
```

### Identify threshold and performance of global ensemble model
```{r evaluate global_ensemble}
global.ens.thresh<-findThresh(global_stack$ens_model$pred)
accuracyStats(global_stack$ens_model$pred,global.ens.thresh$predicted)
```
### Create rasterstack of CHELSA climate data clipped to European modeling extent for prediction

```{r prepareEU_chelsaData,echo=TRUE}
euclimrasters <- list.files((here("./data/external/climate/chelsa_eu_clips")),pattern='tif',full.names = T)
eu_climpreds<-stack(euclimrasters)
eu_climpreds.10<-divide10(eu_climpreds) # correct for integer format of Chelsa preds

```

### Restrict global model prediction to the extent of Europe
```{r predictGlobal, cache=TRUE,echo=FALSE}

 global_model<-raster::predict(eu_climpreds.10,global_stack,type="prob")

```
### Plot global model prediction
```{r plotGlobal,echo=FALSE}
brks <- seq(0, 1, by=0.1) 
  nb <- length(brks)-1 
  pal <- colorRampPalette(rev(brewer.pal(8, 'Spectral')))
  cols<-pal(nb)
  plot(global_model, breaks=brks, col=cols,lab.breaks=brks) 
 plot(global.occ.sp,pch=21,col="white",cex=0.70,add=TRUE)
```
### Export global model prediction
```{r exportGlobal}
writeRaster(global_model, filename=file.path(rasterOutput,paste("GlobalEnsEU_",taxonkey, ".tif",sep="")),format="GTiff",overwrite=TRUE) 
```
### Get variable importance of global model

```{r get_var_importance}
variableImportance_global<-varImp(global_stack)
kable(variableImportance_global,digits=2,caption="Variable Importance") %>%
kable_styling(bootstrap_options = c("striped"))
write.csv(variableImportance_global,file = paste0(genOutput,taxonkey,"_varImp_global_model.csv"))
```
