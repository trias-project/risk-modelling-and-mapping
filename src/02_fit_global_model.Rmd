
---
title: "Untitled"
author: "Amy Davis", 
        "Soria Delva"
date: "2024-08-20"
output: html_document
---

### 2. Create a global SDM

#### To do: Specify project
```{r}
#specify project name
projectname<-"Test_Frédérique"
```

#### Load packages
```{r load libraries,echo=FALSE,message=FALSE}
options("rgdal_show_exportToProj4_warnings"="none")
library(here)
library(dplyr)
library(stringr)
library(CoordinateCleaner)
library(terra)
library(raster)
library(sf)
library(rnaturalearth)
library(ggplot2)
library(tidyterra)
library(mapview)
library(dismo)
library(sdm)
library(caret)
library(viridisLite)
library(kableExtra)
library(future)
library(future.apply)
library(qs)
```

```{r install or load right version of caretEnsemble}
# Define the desired version
desired_version <- "2.0.3"

# Check if caretEnsemble is installed
if ("caretEnsemble" %in% rownames(installed.packages())) {
  # Get the current version of caretEnsemble
  current_version <- packageVersion("caretEnsemble")
  # Compare current version with the desired version
  if (as.character(current_version) != desired_version) {
    # Uninstall the current version if it's not the desired version
    remove.packages("caretEnsemble")
    # Install the specific version
   devtools::install_github("zachmayer/caretEnsemble@2.0.3")
    # Load 
    library(caretEnsemble)
  } else {
    library(caretEnsemble)
  }
  
} else {
  # If caretEnsemble is not installed, install the specific version
 devtools::install_github("zachmayer/caretEnsemble@2.0.3")
  # Load 
  library(caretEnsemble)
  rm(current_version, desired_version)
}
```

####  Source helper functions
```{r source helpers, echo=FALSE}
source("./src/helper_functions.R")
```

#### Load global occurrence data and taxa info
```{r Load data}
global<-qread( paste0("./data/projects/",projectname,"/",projectname,"_occurrences.qs"))
taxa_info<-read.csv2(paste0("./data/projects/",projectname,"/",projectname,"_taxa_info.csv"))
accepted_taxonkeys<-taxa_info%>%
  pull(accepted_taxonkeys)%>%
  unique()
```


####3. Filter global occurrence data 

```{r filter global occurrence data}

#remove unverified records
identificationVerificationStatus_to_discard <- c( "unverified",
                                                  "unvalidated",
                                                  "not validated",
                                                  "under validation",
                                                  "not able to validate",
                                                  "control could not be conclusive due to insufficient knowledge",
                                                  "1",
                                                  "uncertain",
                                                  "unconfirmed",
                                                  "Douteux",
                                                  "Invalide",
                                                  "Non r\u00E9alisable",
                                                  "verification needed" ,
                                                  "Probable",
                                                  "unconfirmed - not reviewed",
                                                  "validation requested")

#enter value for max coordinate uncertainty in meters.
global.occ<-global %>%
  dplyr::filter(speciesKey%in%accepted_taxonkeys) %>%   
  dplyr::filter(is.na(coordinateUncertaintyInMeters)| coordinateUncertaintyInMeters<= 1000) %>%
  dplyr::filter(!str_to_lower(identificationVerificationStatus) %in% identificationVerificationStatus_to_discard)

#Remove coordinates that for both lon and lat values, have less than 4 decimal places
 global.occ$lon_dplaces<-sapply(global.occ$decimalLongitude, function(x) decimalplaces(x))
 global.occ$lat_dplaces<-sapply(global.occ$decimalLatitude, function(x) decimalplaces(x))
 global.occ[global.occ$lon_dplaces < 4 & global.occ$lat_dplaces < 4 , ]<-NA
 global.occ<-global.occ[ which(!is.na(global.occ$lon_dplaces)),]
 global.occ<-within(global.occ,rm("lon_dplaces","lat_dplaces")) # n= 1758
 
``` 

#### Convert global occurrences to spatial points needed for modelling

```{r occ to spatialData,echo=FALSE}
global.occ <- global.occ%>%
  dplyr::mutate(Group = case_when(kingdom == "Plantae" ~ "Plants",
                            class == "Aves" ~ "Birds",
                            phylum == "Mollusca" ~ "Molluscs",
                            class == "Amphibia" ~ "Amphibians",
                            class == "Mammalia" ~ "Mammals",
                            class == "Crocodylia" ~ "Reptiles",
                            class == "Testudines" ~ "Reptiles",
                            class == "Sphenodontia" ~ "Reptiles",
                            class == "Squamata" ~ "Reptiles",
                            TRUE ~ NA_character_))


global.occ.LL<-data.frame(global.occ)[c(4,3,2,1,10)] #dcimalLon, decimalLat, acceptedScientificName, acceptedtaxonkey, Group, n= 1758
global.occ.LL<-rename(global.occ.LL, "species"="acceptedScientificName")
rm(global.occ, global)
```

#### Flag and remove centroids and invalid georeferenced points
```{r remove invalid_pts_and_centroids,echo=FALSE}
# OPTIONAL: Coordinates are tested for several things: whether they are in capitals, whether ... . For each coordinate a column per test is added indicating wether the result is potentially problematic (FALSE) or a clean coordinate (TRUE)
#flags_report<-clean_coordinates(x = global.occ.LL, lon= "decimalLongitude", lat= "decimalLatitude",
                        #  tests = c("capitals", 
                         # "centroids","gbif", "institutions", 
                          # "seas", "zeros"))

# Only keep coordinates that are not flagged as potentially problematic
cleaned<-clean_coordinates(x = global.occ.LL, lon= "decimalLongitude", lat= "decimalLatitude",
                          tests = c("capitals", 
                          "centroids","gbif", "institutions", 
                           "zeros"),value="clean")

```
#### Create global rasterstack using CHELSA data for model building

```{r importChelsaData,echo=TRUE}
globalclimrasters <- list.files((here("./data/external/climate/trias_CHELSA")),pattern='tif',full.names = T) #import CHELSA data
globalclimpreds_terra <- terra::rast(globalclimrasters)
```

#### Create shape of the world
```{r Create shape of world}
world<-ne_countries(scale=50)
```

### Create rasterstack of CHELSA climate data clipped to European modeling extent for prediction

```{r prepareEU_chelsaData,echo=TRUE}
euclimrasters <- list.files((here("./data/external/climate/chelsa_eu_clips")),pattern='tif',full.names = T)
eu_climpreds<-rast(euclimrasters)
eu_climpreds.10<-divide10(eu_climpreds)  # correct for integer format of Chelsa preds
```

### Create a named list where each species group has a filepath to the corresponding biasgrid
```{r create list with biasgrid paths}
bias_grid_paths <- list(
  Plants = here("./data/external/bias_grids/final/trias/plants_1deg_min5.tif"),
  Amphibians = here("./data/external/bias_grids/final/trias/amphib_1deg_min5.tif"),
  Birds = here("./data/external/bias_grids/final/trias/birds_1deg_min5.tif"),
  Mammals = here("./data/external/bias_grids/final/trias/mammals_1deg_min5.tif"),
  Molluscs = here("./data/external/bias_grids/final/trias/molluscs_1deg_min5.tif"),
  Reptiles = here("./data/external/bias_grids/final/trias/reptiles_1deg_min5.tif")
)
```

#split dataframe by taxon key
```{r split_GBIF_occurrence_bySpecies}
sort(unique(cleaned$species))
split_df<-split(cleaned,cleaned$species) 

```

#Remove objects we don't need anymore
```{r remove objects}
rm(global.occ.LL,cleaned)
gc()
```

#Run parallel SDM modelling 
```{r prepare cores to run in parallel, echo=FALSE}
system.time({ #2576.6sec without parallelization for 5 species (43 min)
for(i in seq_along (split_df)){ 
  globalmodels<-list()
  species<-names(split_df)[i]
  first_two_words <- sub("^(\\w+)\\s+(\\w+).*", "\\1_\\2", species)  # Extract first two words of species name
  global.occ.LL.cleaned<-split_df[[i]]
  taxonkey<-unique(global.occ.LL.cleaned$speciesKey)
  speciesgroup<-unique(global.occ.LL.cleaned$Group)
  global.occ.LL.cleaned<-global.occ.LL.cleaned %>%
    dplyr::select(c(decimalLongitude,decimalLatitude))
```

#### Create species folder
```{r}
species_folder<-paste0("./data/projects/",projectname,"/",first_two_words,"_",taxonkey)
if (!dir.exists(species_folder)) {
    dir.create(species_folder, recursive = TRUE)
  }
```

#### Remove duplicates per grid cell and define number of pseudoabsences

```{r remove global duplicates,echo=TRUE}
global.occ.LL.cleaned$cell<-terra::cellFromXY( globalclimpreds_terra, global.occ.LL.cleaned) #Indicate for each occurrence point in which cell of the raster it falls
global.occ.LL.cleaned<-global.occ.LL.cleaned[!is.na(global.occ.LL.cleaned$cell),]
unique_occurrences <- !duplicated(global.occ.LL.cleaned$cell)# Identify unique occurrences
global.occ.LL.cleaned <- global.occ.LL.cleaned[unique_occurrences, 1:2] # Subset the occurrence points to keep only one occurrence per raster cell 

global.occ.LL.cleaned<- terra::extract(globalclimpreds_terra, global.occ.LL.cleaned, xy = T, ID=F)%>%
  dplyr::filter(rowSums(is.na(.[, 1:(ncol(.) - 2)])) == 0)%>% #Keep rows that do not have any NA values in column 1- 3rd last 
  dplyr::select(c(x,y))%>%
  dplyr::rename(decimalLongitude=x,
                decimalLatitude=y) #Extract climatic values of occurrence points from each raster layer and remove occurrence points that fall in cells with NA values in at least one rasterlayer

global.occ.LL.cleaned$species<- rep(1,length(global.occ.LL.cleaned$decimalLongitude)) #adds columns indicating species presence (1) needed for modeling
global.occ.sf<-st_as_sf(global.occ.LL.cleaned, coords=c("decimalLongitude", "decimalLatitude"), crs=4326, remove= FALSE)
numb.global.pseudoabs <-length(global.occ.sf$decimalLongitude) #sets the number of pseudoabsences equal to number of unique presences
rm(global.occ.LL.cleaned)

```
### plot distribution of cleaned global occurrences
```{r plot_global_occ,echo=TRUE, eval=FALSE}
ggplot()+ 
  geom_sf(data = world,  colour = "black", fill = NA)+
  geom_point(data=global.occ.sf, aes(x=decimalLongitude, y= decimalLatitude),  fill="green", shape = 22, colour = "black", size=3)+
  labs(x="Longitude", y="Latitude")+
  theme_bw()
```
### Select wwf ecoregions that contain global occurrence points

```{r select ecoregions,cache= TRUE,echo=FALSE}
wwf_eco<-sf::st_read(here("./data/external/GIS/official/wwf_terr_ecos.shp"))
wwf_eco<-sf::st_transform(wwf_eco, 4326) %>%
          sf::st_make_valid()
occ_ecoIntersect <- sf::st_intersects(wwf_eco,global.occ.sf) 
wwf_ecoSub1<-wwf_eco[lengths(occ_ecoIntersect) > 0,1]
```

### Inspect the ecoregions object

```{r visualize ecoregions,cache= TRUE,echo=FALSE, eval=FALSE}
ggplot()+ 
  geom_sf(data = world,  colour = "black", fill = NA)+
  geom_sf(data=wwf_ecoSub1, fill="#f7786f")+
  labs(x="Longitude", y="Latitude")+
  theme_bw()
```

### Specify and import bias grids for relevant taxonomic group 

```{r importBiasgrid}
if (speciesgroup %in% names(bias_grid_paths)) {
  biasgrid <- terra::rast(bias_grid_paths[[speciesgroup]])
} else {
  stop("No bias grid available for this species. Species has to be one of the following: Plants, Amphibians, Birds, Mammals, Molluscs, or Reptiles.")
}
```

### Subset bias grid by ecoregions containing occurrence points

```{r subsetBiasgrid}
#Mask biasgrid by ecoregions containing occurrence points
wwf_ecoSub1_ext<-terra::ext(wwf_ecoSub1) 
wwf_ecoSub1_vector <- vect(wwf_ecoSub1) #Convert wwf_ecoSub1 to a SpatVector that can be used for masking
biasgrid_crop <- terra::crop(biasgrid, wwf_ecoSub1_ext) #Crop biasgrid to extent wwf_ecoSub1
biasgrid_sub <- terra::mask(biasgrid_crop, wwf_ecoSub1_vector)#Mask cropped biasgrid with SpatVector

#Mask biasgrid with one of the climatic layers, to make sure it doesn't extend beyond them
climategrid_rast<-terra::crop(globalclimpreds_terra[[1]], wwf_ecoSub1_ext)
biasgrid_sub<-terra::mask(biasgrid_sub, climategrid_rast) 

#Visualize
ggplot()+ 
 geom_sf(data = world,  colour = "black", fill = NA)+
  geom_spatraster(data=biasgrid_sub)+
  scale_fill_continuous(na.value = "transparent",low = "blue", high = "orange")+
 labs(x="Longitude", y="Latitude")+
  theme_bw()
 
biasgrid_sub <- raster(biasgrid_sub) #Convert SpatRaster back to normal raster object
```

###  Use randomPoints function from dismo package to locate pseduobasences within the bias grid subset 

```{r locatePseudo_absences}
# generates pseudo absences equal to the number of presences, from cells that are not NA in biasgrid_sub and not from cells that have occurrence points as indicated in global.occ.sf. extf 1.1 increases the size of extent with 5% at each side of the extent (default value) but when ext is NULL it won't do anything.excludep = TRUE indicates that presence points are excluded from the background, prob: if TRUE the values in mask are interpreted as probability weights (only works for rasters with a modest size that can be loaded into RAM)

#Create alternative raster consisting of only ecoregions without biasgrid mask, used only when not enough pseudoabsence points can be generated using biasgrid_sub as mask layer
 ecoregions_crop<-terra::crop(globalclimpreds_terra[[1]], wwf_ecoSub1_ext) #Crop one of the climate rasters to extent ecoregions
  ecoregions_raster<-mask(ecoregions_crop,wwf_ecoSub1_vector) #Mask with ecoregions vector

#Generate pseudoabsences
set.seed(728)
global_points <- generate_pseudoabs( mask = biasgrid_sub, alternative_mask = raster(ecoregions_raster) , n = numb.global.pseudoabs, p =  st_drop_geometry(global.occ.sf))

```


### Extract generated pseudo absences and create presence-pseudobasence dataset 

```{r create_presence_absence_dataset}
global_pseudoAbs<-global_points %>%
  as.data.frame()%>%
  mutate(species = rep(0,length(x)))%>%
  st_as_sf(coords=c("x", "y"), crs=4326, remove=FALSE)%>%
  dplyr::rename(decimalLongitude=x,
         decimalLatitude=y)

global_presabs<- rbind(global.occ.sf,global_pseudoAbs)# join pseudoabsences with presences 
rm(global_points)
```

### Inspect the presences and pseudoabsences

```{r visualize presences and pseudoabsences,cache= TRUE,echo=FALSE , eval=FALSE}
mapview(biasgrid_sub, 
        col.regions = colorRampPalette(c("blue", "orange")),
        alpha=1, 
        na.color = "transparent", 
        layer.name = "Bias Grid") +
mapview(global_presabs, zcol = "species", 
        col.regions = c("red", "yellow"),
        layer.name = "Species distribution")
```

### Extract climate data for global scale modelling

```{r extractClimateData,message=FALSE}
global.data <- sdm::sdmData(species~.,train=vect(global_presabs),predictors=globalclimpreds_terra) 
global.data.df<-as.data.frame(global.data)

```

### Identify highly correlated predictors

```{r identifyCorrelatedPreds,echo=FALSE}
correlationMatrix<-cor(global.data.df[,-c(1,2)]) #Calculate pearson correlation among environmental values
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.7,exact=TRUE,names=TRUE)#Returns names of environmental variables to be removed because they are correlated more than 0.7 with other variables.  If two variables have a high correlation, the function removes the variable with the largest mean absolute correlation.
preds<-as.data.frame(highlyCorrelated)
kable(preds) %>%
  kable_styling(bootstrap_options = c("striped"))
```

### Remove highly correlated predictors from dataframe 

```{r removeCorrelatedPreds,echo=FALSE,message=FALSE,warning=FALSE}
global.data.df.subset<- global.data.df %>%
  select (-all_of(highlyCorrelated), -rID) %>% 
  mutate(species = as.factor(species)) %>%
  mutate(species = recode_factor(species, 
                '0' = "absent",
                '1' = "present")) %>%  # Later steps require non numeric dependent variable
  mutate(species = relevel(species, ref = "present")) 
```
### Correct global clim preds values from integer format

```{r correctPreds,echo=FALSE}
global.data.df.uncor<-cbind("species"=  global.data.df.subset$species,divide10(global.data.df.subset[,-c(1)]))
```


### Use caretList from Caret package to run multiple machine learning models

```{r run_globalModel,cache=TRUE,results= 'hide',message=FALSE,warning=FALSE}
#preProc: preprocessing of predictors (environmental data). method = "center" subtracts the mean of the predictor's data (again from the data in x) from the predictor values while method = "scale" divides by the standard deviation.
control <- caret::trainControl(method="cv",
                               number=10,
                               savePredictions="final", 
                               preProc=c("center","scale"),
                               classProbs=TRUE)
classList1 <- c("glm","gbm","rf","earth")
set.seed(457)
global_train <- caretEnsemble::caretList(species~., 
                                         data= global.data.df.uncor,
                                         trControl=control,
                                         methodList=classList1,
                                         metric="Accuracy")
```

```{r print_globalModelAccuracy,warning=FALSE}
GlobalModelResults<-resamples(global_train) #Returns the results for each model 
Global.Mod.Accuracy<-summary(GlobalModelResults)# displays accuracy of each model
kable(Global.Mod.Accuracy$statistics$Accuracy,digits=2) %>%
  kable_styling(bootstrap_options = c("striped"))
```

```{r print_globalModelKappa,warning=FALSE}
kable(Global.Mod.Accuracy$statistics$Kappa,digits=2) %>%
  kable_styling(bootstrap_options = c("striped"))
```

```{r print_globalModelCorrelation,warning=FALSE}
Global.Mod.Cor<-modelCor(resamples(global_train))# shows correlation among models.Weakly correlated algorithms are persuasive for stacking them in ensemble.
kable(Global.Mod.Cor,digits=2)%>%
  kable_styling(bootstrap_options = c("striped"))
```

### Create ensemble model (combine individual models into one) 
```{r run global_ensemble,TRUE}
set.seed(478)
global_stack <- caretEnsemble(
  global_train, 
  metric="Accuracy",
  trControl=trainControl(method="cv",
                         number=10,
                         savePredictions= "final",
                         classProbs=TRUE))
print(global_stack)
```

### Identify threshold and performance of global ensemble model
```{r evaluate global_ensemble}
global.ens.thresh<-findThresh(global_stack$ens_model$pred)
ensemble_accurracy<-accuracyStats(global_stack$ens_model$pred,global.ens.thresh$predicted)
```

### Remove objects that are not needed anymore
```{r clean up global environment}

#Remove all files except for climate_matching and marine
rm(list = setdiff(ls(), c("eu_climpreds.10", "global_stack", "split_df","pdfOutput","rasterOutput","genOutput", "taxonkey", "species", "ensemble_accurracy", "accuracyStats", "decimalplaces", "divide10", "findThresh", "predict_large_raster", "globalclimpreds_terra","bias_grid_paths", "i", "globalmodels","global.occ.sf", "biasgrid_sub", "world", "projectname", "first_two_words")))
```

### Restrict global model prediction to the extent of Europe
```{r predictGlobal, cache=TRUE,echo=FALSE}

system.time({
 global_model <- predict(eu_climpreds.10,global_stack,type="prob", na.rm = TRUE) #235.05
})

```

### Plot global model prediction
```{r plotGlobal,echo=FALSE, eval=FALSE}
brks <- seq(0, 1, by=0.1) 
  nb <- length(brks)-1 
# Generate Viridis palette
viridis_palette <- viridis(nb)
  
ggplot() + 
  #geom_sf(data = world,  colour = "grey", fill = NA)+
  geom_spatraster(data = global_model) +
  scale_fill_gradientn(colors = viridis_palette, breaks = brks, labels = brks, na.value = NA) +
  geom_sf(data = global.occ.sf, color = "black", fill = "red", size =1.5, shape = 21) +
  coord_sf(xlim = c(-10, 40), ylim = c(35, 72)) + 
  labs(fill = "Suitability")+
  theme_bw()
```

### Get variable importance of global model

```{r get_var_importance}
variableImportance_global<-varImp(global_stack)
kable(variableImportance_global,digits=2,caption="Variable Importance") %>%
  kable_styling(bootstrap_options = c("striped"))
 
```

```{r store results in list, echo=FALSE}
  
globalmodels <-list(species = species,
                    taxonkey = taxonkey,
                    global_ensemble_model = global_stack, 
                    model_accuracy = ensemble_accurracy,
                    variable_importance = variableImportance_global,
                    global_model_predictions = terra::wrap(global_model), #Needs to be wrapped to export it or will return a null pointer error
                    occurrences=global.occ.sf,
                    biasgrid=biasgrid_sub
)

qsave(globalmodels, paste0("./data/projects/",projectname,"/",first_two_words,"_",taxonkey,"/Global_model_",first_two_words,"_",taxonkey,".qs"))

print(paste("Global model has been created for", species))
  
}
})
```
